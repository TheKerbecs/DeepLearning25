{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN_CIFAR10.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: Vision Transformers on CIFAR10"
      ],
      "metadata": {
        "id": "9rZPsOU0phDS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i0AIosM0AaY2"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "from __future__ import print_function\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the dataset\n",
        "dataset = dset.CIFAR10(root=\"./data\", download=True,\n",
        "                           transform=transforms.Compose([\n",
        "                               transforms.Resize(64),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "                           ]))\n",
        "nc=3\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n",
        "                                         shuffle=True, num_workers=2)\n"
      ],
      "metadata": {
        "id": "is2FpH_lAtJJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the availability of cuda devices\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "yF-tedlhAx-s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks:\n",
        "* try to get the best test Accuracy on Cifar10 using a transformer model\n",
        "* pre-trained models allowed - see [here](https://docs.pytorch.org/vision/main/models/vision_transformer.html) for list of models in TorchVision\n",
        "* **hint**: just like with the CNN in Week 5 - wee need to change the classification layer to fit our 10 class CIFAR-10 problem before we can fine-tune it...\n",
        "* **hint**: Transformers need a lot of compute + memory - use the A100 GPU\n",
        "\n"
      ],
      "metadata": {
        "id": "KxRD7Myvpogs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from transformers import ViTForImageClassification, ViTConfig\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Define device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Loading the dataset\n",
        "# We will use a larger image size for the transformer\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),  # ViT models typically use 224x224 input\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "trainset = CIFAR10(root=\"./data\", download=True, train=True, transform=transform)\n",
        "testset = CIFAR10(root=\"./data\", download=True, train=False, transform=transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32  # Adjust based on available GPU memory\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "# Load a pre-trained Vision Transformer model\n",
        "model_name = \"google/vit-base-patch16-224\"\n",
        "model = ViTForImageClassification.from_pretrained(model_name)\n",
        "\n",
        "num_classes = 10\n",
        "model.classifier = nn.Linear(model.config.hidden_size, num_classes)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5) # Lower learning rate for fine-tuning\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Initialize Accelerate\n",
        "accelerator = Accelerator()\n",
        "model, optimizer, trainloader, testloader = accelerator.prepare(\n",
        "    model, optimizer, trainloader, testloader\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Evaluation on the test set\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images).logits\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Epoch {epoch + 1} - Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Final evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images).logits\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "final_accuracy = 100 * correct / total\n",
        "print(f'Final Test Accuracy: {final_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "id": "gMhxLnIeBfBe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e2a58ae-097a-4d71-dece-ff40a362b99c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1, Batch 100] loss: 0.502\n",
            "[Epoch 1, Batch 200] loss: 0.111\n",
            "[Epoch 1, Batch 300] loss: 0.093\n",
            "[Epoch 1, Batch 400] loss: 0.097\n",
            "[Epoch 1, Batch 500] loss: 0.078\n",
            "[Epoch 1, Batch 600] loss: 0.091\n",
            "[Epoch 1, Batch 700] loss: 0.072\n",
            "[Epoch 1, Batch 800] loss: 0.092\n",
            "[Epoch 1, Batch 900] loss: 0.072\n",
            "[Epoch 1, Batch 1000] loss: 0.062\n",
            "[Epoch 1, Batch 1100] loss: 0.087\n",
            "[Epoch 1, Batch 1200] loss: 0.066\n",
            "[Epoch 1, Batch 1300] loss: 0.066\n",
            "[Epoch 1, Batch 1400] loss: 0.055\n",
            "[Epoch 1, Batch 1500] loss: 0.070\n",
            "Epoch 1 - Test Accuracy: 97.57%\n",
            "[Epoch 2, Batch 100] loss: 0.025\n",
            "[Epoch 2, Batch 200] loss: 0.025\n",
            "[Epoch 2, Batch 300] loss: 0.024\n",
            "[Epoch 2, Batch 400] loss: 0.019\n",
            "[Epoch 2, Batch 500] loss: 0.033\n",
            "[Epoch 2, Batch 600] loss: 0.041\n",
            "[Epoch 2, Batch 700] loss: 0.032\n",
            "[Epoch 2, Batch 800] loss: 0.033\n",
            "[Epoch 2, Batch 900] loss: 0.029\n",
            "[Epoch 2, Batch 1000] loss: 0.031\n",
            "[Epoch 2, Batch 1100] loss: 0.034\n",
            "[Epoch 2, Batch 1200] loss: 0.051\n",
            "[Epoch 2, Batch 1300] loss: 0.050\n",
            "[Epoch 2, Batch 1400] loss: 0.039\n",
            "[Epoch 2, Batch 1500] loss: 0.030\n",
            "Epoch 2 - Test Accuracy: 97.96%\n",
            "[Epoch 3, Batch 100] loss: 0.013\n",
            "[Epoch 3, Batch 200] loss: 0.010\n",
            "[Epoch 3, Batch 300] loss: 0.011\n",
            "[Epoch 3, Batch 400] loss: 0.012\n",
            "[Epoch 3, Batch 500] loss: 0.023\n",
            "[Epoch 3, Batch 600] loss: 0.013\n",
            "[Epoch 3, Batch 700] loss: 0.018\n",
            "[Epoch 3, Batch 800] loss: 0.009\n",
            "[Epoch 3, Batch 900] loss: 0.017\n",
            "[Epoch 3, Batch 1000] loss: 0.025\n",
            "[Epoch 3, Batch 1100] loss: 0.031\n",
            "[Epoch 3, Batch 1200] loss: 0.026\n",
            "[Epoch 3, Batch 1300] loss: 0.026\n",
            "[Epoch 3, Batch 1400] loss: 0.018\n",
            "[Epoch 3, Batch 1500] loss: 0.024\n",
            "Epoch 3 - Test Accuracy: 97.37%\n",
            "[Epoch 4, Batch 100] loss: 0.027\n",
            "[Epoch 4, Batch 200] loss: 0.015\n",
            "[Epoch 4, Batch 300] loss: 0.013\n",
            "[Epoch 4, Batch 400] loss: 0.012\n",
            "[Epoch 4, Batch 500] loss: 0.017\n",
            "[Epoch 4, Batch 600] loss: 0.012\n",
            "[Epoch 4, Batch 700] loss: 0.027\n",
            "[Epoch 4, Batch 800] loss: 0.026\n",
            "[Epoch 4, Batch 900] loss: 0.020\n",
            "[Epoch 4, Batch 1000] loss: 0.013\n",
            "[Epoch 4, Batch 1100] loss: 0.019\n",
            "[Epoch 4, Batch 1200] loss: 0.019\n",
            "[Epoch 4, Batch 1300] loss: 0.012\n",
            "[Epoch 4, Batch 1400] loss: 0.015\n",
            "[Epoch 4, Batch 1500] loss: 0.014\n",
            "Epoch 4 - Test Accuracy: 97.33%\n",
            "[Epoch 5, Batch 100] loss: 0.017\n",
            "[Epoch 5, Batch 200] loss: 0.013\n",
            "[Epoch 5, Batch 300] loss: 0.012\n",
            "[Epoch 5, Batch 400] loss: 0.013\n",
            "[Epoch 5, Batch 500] loss: 0.014\n",
            "[Epoch 5, Batch 600] loss: 0.033\n",
            "[Epoch 5, Batch 700] loss: 0.027\n",
            "[Epoch 5, Batch 800] loss: 0.015\n",
            "[Epoch 5, Batch 900] loss: 0.020\n",
            "[Epoch 5, Batch 1000] loss: 0.033\n",
            "[Epoch 5, Batch 1100] loss: 0.014\n",
            "[Epoch 5, Batch 1200] loss: 0.018\n",
            "[Epoch 5, Batch 1300] loss: 0.012\n",
            "[Epoch 5, Batch 1400] loss: 0.006\n",
            "[Epoch 5, Batch 1500] loss: 0.012\n",
            "Epoch 5 - Test Accuracy: 98.11%\n",
            "[Epoch 6, Batch 100] loss: 0.011\n",
            "[Epoch 6, Batch 200] loss: 0.009\n",
            "[Epoch 6, Batch 300] loss: 0.012\n",
            "[Epoch 6, Batch 400] loss: 0.014\n",
            "[Epoch 6, Batch 500] loss: 0.017\n",
            "[Epoch 6, Batch 600] loss: 0.021\n",
            "[Epoch 6, Batch 700] loss: 0.008\n",
            "[Epoch 6, Batch 800] loss: 0.002\n",
            "[Epoch 6, Batch 900] loss: 0.013\n",
            "[Epoch 6, Batch 1000] loss: 0.022\n",
            "[Epoch 6, Batch 1100] loss: 0.018\n",
            "[Epoch 6, Batch 1200] loss: 0.018\n",
            "[Epoch 6, Batch 1300] loss: 0.013\n",
            "[Epoch 6, Batch 1400] loss: 0.016\n",
            "[Epoch 6, Batch 1500] loss: 0.015\n",
            "Epoch 6 - Test Accuracy: 98.08%\n",
            "[Epoch 7, Batch 100] loss: 0.005\n",
            "[Epoch 7, Batch 200] loss: 0.010\n",
            "[Epoch 7, Batch 300] loss: 0.008\n",
            "[Epoch 7, Batch 400] loss: 0.003\n",
            "[Epoch 7, Batch 500] loss: 0.018\n",
            "[Epoch 7, Batch 600] loss: 0.014\n",
            "[Epoch 7, Batch 700] loss: 0.007\n",
            "[Epoch 7, Batch 800] loss: 0.009\n",
            "[Epoch 7, Batch 900] loss: 0.010\n",
            "[Epoch 7, Batch 1000] loss: 0.029\n",
            "[Epoch 7, Batch 1100] loss: 0.021\n",
            "[Epoch 7, Batch 1200] loss: 0.019\n",
            "[Epoch 7, Batch 1300] loss: 0.020\n",
            "[Epoch 7, Batch 1400] loss: 0.013\n",
            "[Epoch 7, Batch 1500] loss: 0.006\n",
            "Epoch 7 - Test Accuracy: 98.05%\n",
            "[Epoch 8, Batch 100] loss: 0.008\n",
            "[Epoch 8, Batch 200] loss: 0.006\n",
            "[Epoch 8, Batch 300] loss: 0.006\n",
            "[Epoch 8, Batch 400] loss: 0.014\n",
            "[Epoch 8, Batch 500] loss: 0.006\n",
            "[Epoch 8, Batch 600] loss: 0.024\n",
            "[Epoch 8, Batch 700] loss: 0.017\n",
            "[Epoch 8, Batch 800] loss: 0.010\n",
            "[Epoch 8, Batch 900] loss: 0.010\n",
            "[Epoch 8, Batch 1000] loss: 0.010\n",
            "[Epoch 8, Batch 1100] loss: 0.013\n",
            "[Epoch 8, Batch 1200] loss: 0.014\n",
            "[Epoch 8, Batch 1300] loss: 0.017\n",
            "[Epoch 8, Batch 1400] loss: 0.006\n",
            "[Epoch 8, Batch 1500] loss: 0.024\n",
            "Epoch 8 - Test Accuracy: 97.80%\n",
            "[Epoch 9, Batch 100] loss: 0.009\n",
            "[Epoch 9, Batch 200] loss: 0.006\n",
            "[Epoch 9, Batch 300] loss: 0.010\n",
            "[Epoch 9, Batch 400] loss: 0.006\n",
            "[Epoch 9, Batch 500] loss: 0.008\n",
            "[Epoch 9, Batch 600] loss: 0.013\n",
            "[Epoch 9, Batch 700] loss: 0.007\n",
            "[Epoch 9, Batch 800] loss: 0.008\n",
            "[Epoch 9, Batch 900] loss: 0.009\n",
            "[Epoch 9, Batch 1000] loss: 0.018\n",
            "[Epoch 9, Batch 1100] loss: 0.011\n",
            "[Epoch 9, Batch 1200] loss: 0.008\n",
            "[Epoch 9, Batch 1300] loss: 0.020\n",
            "[Epoch 9, Batch 1400] loss: 0.020\n",
            "[Epoch 9, Batch 1500] loss: 0.018\n",
            "Epoch 9 - Test Accuracy: 97.22%\n",
            "[Epoch 10, Batch 100] loss: 0.007\n",
            "[Epoch 10, Batch 200] loss: 0.007\n",
            "[Epoch 10, Batch 300] loss: 0.010\n",
            "[Epoch 10, Batch 400] loss: 0.003\n",
            "[Epoch 10, Batch 500] loss: 0.006\n",
            "[Epoch 10, Batch 600] loss: 0.001\n",
            "[Epoch 10, Batch 700] loss: 0.005\n",
            "[Epoch 10, Batch 800] loss: 0.006\n",
            "[Epoch 10, Batch 900] loss: 0.021\n",
            "[Epoch 10, Batch 1000] loss: 0.026\n",
            "[Epoch 10, Batch 1100] loss: 0.015\n",
            "[Epoch 10, Batch 1200] loss: 0.012\n",
            "[Epoch 10, Batch 1300] loss: 0.007\n",
            "[Epoch 10, Batch 1400] loss: 0.015\n",
            "[Epoch 10, Batch 1500] loss: 0.015\n",
            "Epoch 10 - Test Accuracy: 98.23%\n",
            "Finished Training\n",
            "Final Test Accuracy: 98.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m_Dbc3R6vENq"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}